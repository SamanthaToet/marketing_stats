---
title: "Marketing Analytics with R: Statistical Modeling"
author: "Samantha Toet"
date: "7/11/2018"
output: html_document
---

These are the notes and practice exercises from the Data Camp Course [Marketing Analytics with R: Statistical Modeling](https://www.datacamp.com/courses/marketing-analytics-in-r-statistical-modeling?utm_source=blog&utm_medium=community&utm_campaign=course_6027)

## Customer Lifetime Value (CLV)

CLV is a prediction of the net profit attributed to the entire future relationship with a customer.

Once the CLV has been calculated, you can use it for:

- ID customers that are more likely to generate higher profits
- target & prioritize customers according to future margins
- no further customer segmentation

Net profit = **Margin**

Plot correlations using `corrplot` package (dark blue = r is closer to 1)

### Simple Linear Regression:

- only use one variable for the independent predictor 
- least squares estimation finds regression line and returns coefficients
- prediction error or residual value is the difference between a point and the line
- specify linear regression model using a formula object in `lm` function:

```{r, eval = FALSE, echo = TRUE}
# predict futureMargin from margin and store that model in simpleLM
simpleLM <- lm(futureMargin ~ margin, data = clvData1)  
summary(simpleLM) # to get an overview of results 
```

Interpreting the results:

- If **coefficient estimate** is > 0, the higher the margin in year 1, then the higher we expect the future margin to be
- The **Multiple R Squared value** is the percentage of variation in the futureMargin that can be explained by the margin in year 1 
- Can be visualized using `geom_point` and `geom_smooth`

Requirements for Simple Linear Regression:

- linear relationship b/w x and y
- no measurement error in x (weak **exogeneity**)
- independence of errors (residuals uncorrelated -should randomly vary around 0)
- expectation of errors is 0
- constant variance of prediction errors (**homoscedasticity**)
- normality of errors (for significance testing)

### Multiple Linear Regression

**Omitted variable bias** is a threat to simple linear regression and occurs when a variable not included in the regression is correlated with both the explanatory variable and the response variable. For example, IQ may be considered an omitted variable when it comes to how study time relates to test performance. 

**Multicollinearity** is a threat to multiple linear regression. It occurs whenever one explanatory variable can be explained by the remaining explanatory variables. It causes standard errors are underestimates. 

- To check all variables in a model for multicollinearity, calculate the **variance inflation factors** using `rms::vif`
- An increate in the variance of an estimated coeffiecient due to multicollinearity 
- VIF > 5 is problematic and VIF > 10 indicates poor regression estimates
- Exclude one of each pair from the regression

Interpretation of coefficients:

- It's hard to make interpredations using just the intercept
- The coefficient of each explanatory variable gives the effect that a one unit change in that variable has on the expected margin (with all other variables being held contstant)

### Model Valdiation, Model Fit, and Prediction

Goodness of fit measueres are used to judge a model's fit. One of them is the **coefficient of determination** or multiple R squared, \( R^2 \) which provides the proportion of the dependent variable's variance that is explained by the regression model, adjusted for the number of variables in the model. 

So if \(R^2\) is 0 none of the variation is explained (i.e. there is no trend in the model), and if \(R^2\) is 1 means that the model explains 100% of the dependent variable's variation.

The **F-test** is a test for the overall fit of the model. It tests if \(R^2\) is 0, or if at least one regressor has significant explanatory power. If `p < o.o5` the hypothesis that \(R^2\) equals 0 is rejected. 

The above are all in-sample measures, i.e. the model is evaluated on the same data it was fitted on, which bears the risk of **overfitting**. 

Ways to avoid overfitting:

- keep your model lean
- `stats::AIC()` - compares two models and penalizes for every explanatory variable. When comparing 2 models, the AIC-minimizing model is preferred. 
- `mass::stepAIC()` - automatic model selection 
- out-of-sample model validation
- cross validation

Use `predict(model)` for making predictions. 


# Churn Prevention with Logistic Regression

Example: you want to predict the probability of a customer returing to your online shop. To do this, you use **binary logistic regression**. Let's look a little deeper:

- The measure of interest is the probability of a customer churning, \(P(Y = 1)\), is difficult to model directly. If you use a linear model you can end up with probabilities outside the range of 0-1
- Model the log odds:
$$ log \frac{P(Y = 1)}{P(Y = 0)} = \beta_0 + \sum^P_{p = 1} \beta_p x_p $$
- Removing the log by using the exponential function gives us the Odds, which is the probability to churn divided by the probability not to churn:
$$ \frac{P(Y = 1)}{P(Y = 0)} = e^Z, Z = \beta_0 + \sum^P_{p = 1} \beta_px_p $$
- The final model for the **probability of a customer churning** is below. It gives us the probability of the target variable being equal to 1. 
$$ P(Y = 1) = \frac{e^Z}{1 + e^Z} $$

### Modeling

To estimate a logistic regression model, use `stats::glm()` and set `family = binomial`. The `summary()` output on your model gives you the estimated coefficients, standard errors, test statistics, and p values. Note that three stars indicate the coefficient is highly significant. The AIC value is also displayed.

`glm()` stands for **generalized linear model** and offers a family of regression models. 

Hypothesis testing: if the null hypothesis (H0) is correct, that means the variable does NOT have an influence on the return of a customer. 

Interpretations of the coefficients is not straightforward. W/o transformation they only indicate the effects on the log odds (i.e. you can only draw conclusions about the direction of that effect). You'll need to extract coefficients using the `coef()` function and transform using the `exp()` function:

```{r eval=FALSE}
coefsExp <- coef(logitModeFull) %>% exp() %>% round(2)
```

To interpret the results, look at the explanatory variable value. If it's greater than 1, for example 1.69, then the results have increased one fold by 69%. 


### Model Selection

When building a model you have to figure out which vartiables to include. One useful tool is `MASS::stepAIC` where the full model is iteratively compares to several other models such that variables are dropped and added based on their significance. It stops when a min AIC is reached. 

Questions to ask after looking at the results of a Step-AIC Function:

- Are there any variables not included in the model that you would have expected to be?
- Are there any variables included that don't make sense?

### In-Sample Model Fit and Thresholding

Measures of model fit use **Pseudo \(R^2\) Statistics** such as McFadden's, Cox & Snell's, and Negelkerke's formulas. For these statistics values greater than 0.2 classify a model as reasonable, greater than 0.4 as good, and greater than 0.5 as very good. 

`descr::logRegR2()` gives multiple goodness of fit measures (inc. above)

Another goodness of fit measure is **accuracy**, which puts the correct predictions in relation to the overall number of observations. First predict the probabilities by using `SDMTools::predict()` and setting `type = response`, and exclude observations with missing values by setting `na.action = na.exclue`. Then display the actual observed values and new predictions using `select()` and `tail()`.

Now observations are classified according to a certain threshols and the predicted and observed outcomes are compared in a **confusion matrix**. Use `SDMTools::confusion.matrix()` with actual observed classes and predicted probabilities:

```{r eval=FALSE}
confMatrixNew <- confusion.matrix(chrunData$returnCustomer, #actual
                                  churnData$predNew, #predicted
                                  threshold = 0.5) #default
```

A classification is correct if it predicts an observation to be 0 where it's true value is 0 (true neg), or if it predicts an observation to be 1 where it's true value is 1 (true pos). In all other cases the prediction is wrong and the observations are misclassified. 

Then check the accuracy of those results:

```{r eval=FALSE}
accuracyNew <- sum(diag(confMatrixNew)) / sum(confMatrixNew)
```

Be careful when interpreting this value. With a threshold of 0.5 there might be some errors. 

### Thresholding

Finding the optimal threshold. For example, if a customer returned based on our prediction, he was sent a coupon, the payoff (how much money you get or lose) is supposed to be 5. However if we wrongly classify someone as churning, whereas they would have returned anyways, leads to a payoff of -15. Predicting a return customer does not cause any direct costs. The payoff is dependent on the true positives and false negatives. Changing the threshold changes the payoff. While the accuracy decreases with a lower threshold, the opposite happens with payoff, reaching a max when threshold = 0. 

**Payoff = x \(*\) true negative - y \(*\) false positive**

Also:

```{r eval=FALSE}
payoffMatrix <- data.frame(threshold = seq(from = 0.1, to = 0.5, by = 0.1),
                           payoff = NA) 
payoffMatrix
```

### Out-of-Sample Validation 

Used to avoid overfitting.

Out-of-Sample Fit: Training and Testing Data

1. Divide the dataset into testing and training data. The training set should make up 2/3rds of the data and testing into 1/3. The goodness of fit measures are calculated on the testing set. 

```{r eval=FALSE}
# set seed to ensure reprex
set.seed(123456)

churnData$isTrain <- rbinom(nrow(churnData), 1, 0.66)
train <- subset(churnData, churnData$isTrain == 1)
test <- subset(churnData, churnData$isTrain == 0)
```

2. Build a model based on training data. Then hand the coefficients of the model to the `predict` function to make predictions. By setting `type = "response"` we predict the probability of the person churning. 

```{r eval=FALSE}
# model logitTrainNew
logitTrainNew <- glm(returnCustomer ~ title + newsletter + websiteDesign +
                         paymentMethod + couponDiscount + purchaseValue + 
                         throughAffiliate + shippingFees + dvd + blueray + vinyl + 
                         videogames + prodRemitted, family = binomial, data = train)

# out-of-sample prediction for logitTrainNew
test$predNew <- predict(logitTrainNew, type = "response", newdata = test)
```

3. Verify and test the model witgh either out-of-sample accuracy or cross validation. To use out-of-sample accuracy:

```{r eval=FALSE}
#calculate the confusion matrix
confMatrixNew <- confusion.matrix(test$returnCustomer, test$predNew, 
                                  threshold = 0.3)
confMatrixNew

#calculate the accuracy
accuracyNew <- sum(diag(confMatrixNew) / sum(confMatrixNew))
accuracyNew
```

### Cross Validation 

An even better tool for overfitting since it needs less data. 

The four-fold cross validation procedure:

1. Split your dataset randomly into 4 subsets. Three will be used for training and one for testing. The test data is used to calculate the model goodness of fit measures

2. Then perform three repetitions, and each time a different subset is used as test data and the remaining three subsets as training data. 

3. Finally you calculate the average goodness of fit measures across the four outputs. 

## Modeling Time to Order with Survival Analysis

**Survival analysis** allows us to model the time to an event, also called failure or survival time. 

Advantages of using a survival model:

- less aggregation (avoids loss of info due to aggregation)

- allows us to model WHEN an event takes place and not just IF it will 

- no arbitratily set timeframe 

- deeper insights into customer relations

**Censoring** is a special case of missing data. **Random type 1 right censoring** means that a subjects event can only be observed if it occurs before a certain point in time (EX. when predicting churn, it must have occured before the current point in time to be observed. We do know, however, that a person has not churned in a certain time frame). Censoring times can very between subjects. 

There are two pieces of info needed for survival analysis: the **time under observation** and the **status at the end of this time**. According to the status we can conclude if an observation was censored or not. 

### Survival Curve Analysis by Kaplan-Meier

The first step is to create a new column that holds a survival object. This will be the dependent variable. To do this we select the relevent two variables (tenure and churn), and use `survival::Surv` to create the survial object and store that in the third column. 

```{r eval=FALSE}
cbind(dataSurv %>% select(tenure, churn),
      surv = Surv(dataSurv$tenure, dataSurv$churn))
```

The measure of interest in a survival analysis is the **survival function**. This function gives the probability that a customer will not churn in the period leading up to time point, t. 

The counterpart to the survival function is the **cumulative hazard function**, which describes the cumulative risk, or the probability that the customer will have churned, up until time, t. 

The **hazard rate**, also called the force of mortality or instantaneous event rate, describes the risk that an event will occur in a small time interval around t, given that the event hasn't happened yet. 

A part of survival analysis is concerned with its estimation. W/o censoring, the estimation of the survival function would be just the percentage of customers who haven't churned at each respective timepoint. The **Kaplan-Meier Estimator** takes into account the number of customers who have churned and the number at risk (customers who are still under contract and might churn). We use `survival::survfit` to estimate the survival function. The survival object is used as the dependent variable here. 

```{r eval=FALSE}
fitKM <- survfit(Surv(dataSurv$tenure, dataSurv$churn) ~ 1, # since we aren't looking at covariants ~ 1
                 type = "kaplan-meier")
fitKM$surv
```

The values of the survival function at diff time points are stored in the `surv` element. 

If we print the survfit object (EX. `print(fitKM)`) we are able to then analyze the object given,

- `n`: number of customers in the dataset

- `events`: number of customers who churned under the time of observation 

- `median`: the median survival time (as in 50% of cust. do not churn before they reach a tenure of `median` value). The median survival time is the time where a horizontal line at 0.5 intersects the survival curve. 

Plotting the survfit object shows overview of function and it's confidence intervals. 

If we want to use categorical covariates (EX. whether or not the customer has a partner), we would include it in creating the survfit object and run the same analyses:

```{r eval=FALSE}
fitKMstr <- survfit(Surv(tenure, churn) ~ Partner,
                    data = dataSurv)
print(fitKMstr)
```

Values of `NA` can mean that the value falls outside of the observational period  without covariates (i.e. outside of the upper and lower 0.95 confidence limits of the first model). This happens if only a few customers churned under the time of observation. 

### Cox Proportional Hazards Model with Constant Covariates

Model assumptions:

- Model definition: \(\lambda(t|x) = \lambda(t) * exp(x'/\beta)\) The predictors are linearly and additively related to the log hazard, so our model follows this form (where \(\lambda\) is the hazard function). 

- No shape of underlying hazard \(\lambda(t)\) assumed. 

- The **proportional hazards function** says that the predictors are not allowed to interact with time. Hence the relative hazard function, \(exp(x'\beta\), must be constant over time. 

Fitting a Survival Model:

First we specify the units that time is measured in. Then for summary stats and predictions, we need to determine the distributions of the predictor variables using `rms::datadist`. 

```{r, eval=FALSE}
units(dataSurv$tenure) <- "Month"
dd <- datadist(dataSurv)
options(datadist = "dd") #accessible in global envi
```

Then we specify the model using `rms::cph`. This is a slight modification of `survival::coxph`. On the left hand side of the formula we want the survival object and on the right the variables are added. 

```{r, eval=FALSE}
fitCHP1 <- cph(Surv(tensure, churn) ~ gender + SeniorCitizen + Partner + Dependents + StreamMov + 
                       PaperlessBilling + PayMeth + MonthlyCharges,
               data = dataSurv,
               x = TRUE, y = TRUE, surv = TRUE,
               time.inc = 1)
```





