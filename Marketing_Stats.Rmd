---
title: "Marketing Analytics with R: Statistical Modeling"
author: "Samantha Toet"
date: "completed 3/28/2019"
output: html_document
---

These are the notes and practice exercises from the Data Camp Course [Marketing Analytics with R: Statistical Modeling](https://www.datacamp.com/courses/marketing-analytics-in-r-statistical-modeling?utm_source=blog&utm_medium=community&utm_campaign=course_6027)

## Customer Lifetime Value (CLV)

CLV is a prediction of the net profit attributed to the entire future relationship with a customer.

Once the CLV has been calculated, you can use it for:

- ID customers that are more likely to generate higher profits
- target & prioritize customers according to future margins
- no further customer segmentation

Net profit = **Margin**

Plot correlations using `corrplot` package (dark blue = r is closer to 1)

### Simple Linear Regression:

- only use one variable for the independent predictor 
- least squares estimation finds regression line and returns coefficients
- prediction error or residual value is the difference between a point and the line
- specify linear regression model using a formula object in `lm` function:

```{r, eval = FALSE, echo = TRUE}
# predict futureMargin from margin and store that model in simpleLM
simpleLM <- lm(futureMargin ~ margin, data = clvData1)  
summary(simpleLM) # to get an overview of results 
```

Interpreting the results:

- If **coefficient estimate** is > 0, the higher the margin in year 1, then the higher we expect the future margin to be
- The **Multiple R Squared value** is the percentage of variation in the futureMargin that can be explained by the margin in year 1 
- Can be visualized using `geom_point` and `geom_smooth`

Requirements for Simple Linear Regression:

- linear relationship b/w x and y
- no measurement error in x (weak **exogeneity**)
- independence of errors (residuals uncorrelated -should randomly vary around 0)
- expectation of errors is 0
- constant variance of prediction errors (**homoscedasticity**)
- normality of errors (for significance testing)

### Multiple Linear Regression

**Omitted variable bias** is a threat to simple linear regression and occurs when a variable not included in the regression is correlated with both the explanatory variable and the response variable. For example, IQ may be considered an omitted variable when it comes to how study time relates to test performance. 

**Multicollinearity** is a threat to multiple linear regression. It occurs whenever one explanatory variable can be explained by the remaining explanatory variables. It causes standard errors are underestimates. 

- To check all variables in a model for multicollinearity, calculate the **variance inflation factors** using `rms::vif`
- An increate in the variance of an estimated coeffiecient due to multicollinearity 
- VIF > 5 is problematic and VIF > 10 indicates poor regression estimates
- Exclude one of each pair from the regression

Interpretation of coefficients:

- It's hard to make interpredations using just the intercept
- The coefficient of each explanatory variable gives the effect that a one unit change in that variable has on the expected margin (with all other variables being held contstant)

### Model Valdiation, Model Fit, and Prediction

Goodness of fit measueres are used to judge a model's fit. One of them is the **coefficient of determination** or multiple R squared, \( R^2 \) which provides the proportion of the dependent variable's variance that is explained by the regression model, adjusted for the number of variables in the model. 

So if \(R^2\) is 0 none of the variation is explained (i.e. there is no trend in the model), and if \(R^2\) is 1 means that the model explains 100% of the dependent variable's variation.

The **F-test** is a test for the overall fit of the model. It tests if \(R^2\) is 0, or if at least one regressor has significant explanatory power. If `p < o.o5` the hypothesis that \(R^2\) equals 0 is rejected. 

The above are all in-sample measures, i.e. the model is evaluated on the same data it was fitted on, which bears the risk of **overfitting**. 

Ways to avoid overfitting:

- keep your model lean
- `stats::AIC()` - compares two models and penalizes for every explanatory variable. When comparing 2 models, the AIC-minimizing model is preferred. 
- `mass::stepAIC()` - automatic model selection 
- out-of-sample model validation
- cross validation

Use `predict(model)` for making predictions. 


# Churn Prevention with Logistic Regression

Example: you want to predict the probability of a customer returing to your online shop. To do this, you use **binary logistic regression**. Let's look a little deeper:

- The measure of interest is the probability of a customer churning, \(P(Y = 1)\), is difficult to model directly. If you use a linear model you can end up with probabilities outside the range of 0-1
- Model the log odds:
$$ log \frac{P(Y = 1)}{P(Y = 0)} = \beta_0 + \sum^P_{p = 1} \beta_p x_p $$
- Removing the log by using the exponential function gives us the Odds, which is the probability to churn divided by the probability not to churn:
$$ \frac{P(Y = 1)}{P(Y = 0)} = e^Z, Z = \beta_0 + \sum^P_{p = 1} \beta_px_p $$
- The final model for the **probability of a customer churning** is below. It gives us the probability of the target variable being equal to 1. 
$$ P(Y = 1) = \frac{e^Z}{1 + e^Z} $$

### Modeling

To estimate a logistic regression model, use `stats::glm()` and set `family = binomial`. The `summary()` output on your model gives you the estimated coefficients, standard errors, test statistics, and p values. Note that three stars indicate the coefficient is highly significant. The AIC value is also displayed.

`glm()` stands for **generalized linear model** and offers a family of regression models. 

Hypothesis testing: if the null hypothesis (H0) is correct, that means the variable does NOT have an influence on the return of a customer. 

Interpretations of the coefficients is not straightforward. W/o transformation they only indicate the effects on the log odds (i.e. you can only draw conclusions about the direction of that effect). You'll need to extract coefficients using the `coef()` function and transform using the `exp()` function:

```{r eval=FALSE}
coefsExp <- coef(logitModeFull) %>% exp() %>% round(2)
```

To interpret the results, look at the explanatory variable value. If it's greater than 1, for example 1.69, then the results have increased one fold by 69%. 


### Model Selection

When building a model you have to figure out which vartiables to include. One useful tool is `MASS::stepAIC` where the full model is iteratively compares to several other models such that variables are dropped and added based on their significance. It stops when a min AIC is reached. 

Questions to ask after looking at the results of a Step-AIC Function:

- Are there any variables not included in the model that you would have expected to be?
- Are there any variables included that don't make sense?

### In-Sample Model Fit and Thresholding

Measures of model fit use **Pseudo \(R^2\) Statistics** such as McFadden's, Cox & Snell's, and Negelkerke's formulas. For these statistics values greater than 0.2 classify a model as reasonable, greater than 0.4 as good, and greater than 0.5 as very good. 

`descr::logRegR2()` gives multiple goodness of fit measures (inc. above)

Another goodness of fit measure is **accuracy**, which puts the correct predictions in relation to the overall number of observations. First predict the probabilities by using `SDMTools::predict()` and setting `type = response`, and exclude observations with missing values by setting `na.action = na.exclue`. Then display the actual observed values and new predictions using `select()` and `tail()`.

Now observations are classified according to a certain threshols and the predicted and observed outcomes are compared in a **confusion matrix**. Use `SDMTools::confusion.matrix()` with actual observed classes and predicted probabilities:

```{r eval=FALSE}
confMatrixNew <- confusion.matrix(chrunData$returnCustomer, #actual
                                  churnData$predNew, #predicted
                                  threshold = 0.5) #default
```

A classification is correct if it predicts an observation to be 0 where it's true value is 0 (true neg), or if it predicts an observation to be 1 where it's true value is 1 (true pos). In all other cases the prediction is wrong and the observations are misclassified. 

Then check the accuracy of those results:

```{r eval=FALSE}
accuracyNew <- sum(diag(confMatrixNew)) / sum(confMatrixNew)
```

Be careful when interpreting this value. With a threshold of 0.5 there might be some errors. 

### Thresholding

Finding the optimal threshold. For example, if a customer returned based on our prediction, he was sent a coupon, the payoff (how much money you get or lose) is supposed to be 5. However if we wrongly classify someone as churning, whereas they would have returned anyways, leads to a payoff of -15. Predicting a return customer does not cause any direct costs. The payoff is dependent on the true positives and false negatives. Changing the threshold changes the payoff. While the accuracy decreases with a lower threshold, the opposite happens with payoff, reaching a max when threshold = 0. 

**Payoff = x \(*\) true negative - y \(*\) false positive**

Also:

```{r eval=FALSE}
payoffMatrix <- data.frame(threshold = seq(from = 0.1, to = 0.5, by = 0.1),
                           payoff = NA) 
payoffMatrix
```

### Out-of-Sample Validation 

Used to avoid overfitting.

Out-of-Sample Fit: Training and Testing Data

1. Divide the dataset into testing and training data. The training set should make up 2/3rds of the data and testing into 1/3. The goodness of fit measures are calculated on the testing set. 

```{r eval=FALSE}
# set seed to ensure reprex
set.seed(123456)

churnData$isTrain <- rbinom(nrow(churnData), 1, 0.66)
train <- subset(churnData, churnData$isTrain == 1)
test <- subset(churnData, churnData$isTrain == 0)
```

2. Build a model based on training data. Then hand the coefficients of the model to the `predict` function to make predictions. By setting `type = "response"` we predict the probability of the person churning. 

```{r eval=FALSE}
# model logitTrainNew
logitTrainNew <- glm(returnCustomer ~ title + newsletter + websiteDesign +
                         paymentMethod + couponDiscount + purchaseValue + 
                         throughAffiliate + shippingFees + dvd + blueray + vinyl + 
                         videogames + prodRemitted, family = binomial, data = train)

# out-of-sample prediction for logitTrainNew
test$predNew <- predict(logitTrainNew, type = "response", newdata = test)
```

3. Verify and test the model witgh either out-of-sample accuracy or cross validation. To use out-of-sample accuracy:

```{r eval=FALSE}
#calculate the confusion matrix
confMatrixNew <- confusion.matrix(test$returnCustomer, test$predNew, 
                                  threshold = 0.3)
confMatrixNew

#calculate the accuracy
accuracyNew <- sum(diag(confMatrixNew) / sum(confMatrixNew))
accuracyNew
```

### Cross Validation 

An even better tool for overfitting since it needs less data. 

The four-fold cross validation procedure:

1. Split your dataset randomly into 4 subsets. Three will be used for training and one for testing. The test data is used to calculate the model goodness of fit measures

2. Then perform three repetitions, and each time a different subset is used as test data and the remaining three subsets as training data. 

3. Finally you calculate the average goodness of fit measures across the four outputs. 

## Modeling Time to Order with Survival Analysis

**Survival analysis** allows us to model the time to an event, also called failure or survival time. 

Advantages of using a survival model:

- less aggregation (avoids loss of info due to aggregation)

- allows us to model WHEN an event takes place and not just IF it will 

- no arbitratily set timeframe 

- deeper insights into customer relations

**Censoring** is a special case of missing data. **Random type 1 right censoring** means that a subjects event can only be observed if it occurs before a certain point in time (EX. when predicting churn, it must have occured before the current point in time to be observed. We do know, however, that a person has not churned in a certain time frame). Censoring times can very between subjects. 

There are two pieces of info needed for survival analysis: the **time under observation** and the **status at the end of this time**. According to the status we can conclude if an observation was censored or not. 

### Survival Curve Analysis by Kaplan-Meier

The first step is to create a new column that holds a survival object. This will be the dependent variable. To do this we select the relevent two variables (tenure and churn), and use `survival::Surv` to create the survial object and store that in the third column. 

```{r eval=FALSE}
cbind(dataSurv %>% select(tenure, churn),
      surv = Surv(dataSurv$tenure, dataSurv$churn))
```

The measure of interest in a survival analysis is the **survival function**. This function gives the probability that a customer will not churn in the period leading up to time point, t. 

The counterpart to the survival function is the **cumulative hazard function**, which describes the cumulative risk, or the probability that the customer will have churned, up until time, t. 

The **hazard rate**, also called the force of mortality or instantaneous event rate, describes the risk that an event will occur in a small time interval around t, given that the event hasn't happened yet. 

A part of survival analysis is concerned with its estimation. W/o censoring, the estimation of the survival function would be just the percentage of customers who haven't churned at each respective timepoint. The **Kaplan-Meier Estimator** takes into account the number of customers who have churned and the number at risk (customers who are still under contract and might churn). We use `survival::survfit` to estimate the survival function. The survival object is used as the dependent variable here. 

```{r eval=FALSE}
fitKM <- survfit(Surv(dataSurv$tenure, dataSurv$churn) ~ 1, # since we aren't looking at covariants ~ 1
                 type = "kaplan-meier")
fitKM$surv
```

The values of the survival function at diff time points are stored in the `surv` element. 

If we print the survfit object (EX. `print(fitKM)`) we are able to then analyze the object given,

- `n`: number of customers in the dataset

- `events`: number of customers who churned under the time of observation 

- `median`: the median survival time (as in 50% of cust. do not churn before they reach a tenure of `median` value). The median survival time is the time where a horizontal line at 0.5 intersects the survival curve. 

Plotting the survfit object shows overview of function and it's confidence intervals. 

If we want to use categorical covariates (EX. whether or not the customer has a partner), we would include it in creating the survfit object and run the same analyses:

```{r eval=FALSE}
fitKMstr <- survfit(Surv(tenure, churn) ~ Partner,
                    data = dataSurv)
print(fitKMstr)
```

Values of `NA` can mean that the value falls outside of the observational period  without covariates (i.e. outside of the upper and lower 0.95 confidence limits of the first model). This happens if only a few customers churned under the time of observation. 

### Cox Proportional Hazards Model with Constant Covariates

Model assumptions:

- Model definition: \(\lambda(t|x) = \lambda(t) * exp(x'/\beta)\) The predictors are linearly and additively related to the log hazard, so our model follows this form (where \(\lambda\) is the hazard function). 

- No shape of underlying hazard \(\lambda(t)\) assumed. 

- The **proportional hazards function** says that the predictors are not allowed to interact with time. Hence the relative hazard function, \(exp(x'\beta\), must be constant over time. 

Fitting a Survival Model:

First we specify the units that time is measured in. Then for summary stats and predictions, we need to determine the distributions of the predictor variables using `rms::datadist`. 

```{r, eval=FALSE}
units(dataSurv$tenure) <- "Month"
dd <- datadist(dataSurv)
options(datadist = "dd") #accessible in global envi
```

Then we specify the model using `rms::cph`. This is a slight modification of `survival::coxph`. On the left hand side of the formula we want the survival object and on the right the variables are added. 

```{r, eval=FALSE}
fitCHP1 <- cph(Surv(tensure, churn) ~ gender + SeniorCitizen + Partner + Dependents + StreamMov + 
                       PaperlessBilling + PayMeth + MonthlyCharges,
               data = dataSurv,
               x = TRUE, y = TRUE, surv = TRUE,
               time.inc = 1)
```

Printing the results gives descriptive stats, goodness of fit measures, and the coefficients and their significance. 

Coefficients are stored in the element named coefficients of the fitted model and are interpreted similarly to a logistic regression. From the untransformed coefficient, we can only draw conclusions about the *direction* of the effect (i.e. positive or negative compared to others). 

We transform the coefficients using the exponential function, ex. `exp(fitCPH1$coefficients)`. This value is called the **hazard ratio**. For example, if the transformed value of "Senior Citizens" was 1.23, then the hazard to churn increases by a factor of 1.23 (or 23%) for senior citizens compared to non-senior citizens. In this case 1.23 is called the hazard ratio. 

For continuous covariates, a one-unit increase in one of these variables influences (decreases, in this example) the hazard of churning by a factor of the exponential value. For example, if the exponential value of "Monthly Charges" was 0.99, then a one unit increase in "Monthly Charges" decreases the hazard of churning by a factor of 0.99. 

Use `rms::survplot` to visualize the predictor effects. This plots the survival probability depending on different levels of one variable while holding the other coefficients constant. 

```{r eval=FALSE}
survplot(fitCHP1, MonthlyCharges, label.curves = list(keys = 1:5))

survplot(fitCHP1, Partner)
```

To visualize the hazard ratios of the coefficients: `plot(summary(fitCPH1)), log = TRUE`

### Checking Model Assumptions and Making Predictions 

To validate your proportional hazards assumption, use `survival::cox.zph` then print:

```{r eval=FALSE}
testCPH1 <- cox.zph(fitCHP1)
print(testCPH1)
```

If the p-value of the test is < 0.05, then we can reject the hypothesis that the given variable meets the proportional hazards assumption. If a predictor violates the proportional hazard assumption (hence their effect changes over time). 

Visualizing the estimates of the coefficient beta(t) dependent on time gives further insights. If the PH assumption holds, beta(t) is a horizontal line. 

General remarks on tests:

- The test provided by the `cox.zph` function is conservative and sensitive to the number of observations. 

- A violation where the coefficient changes signs is worse than one where the coefficient varies between some positive values. 

What if PH assumption is violated?

- If the PH assumption is violated for a certain variable, a **stratified cox model** makes sense. This model allows the shape of the underlying hazard to vary for the diff levels of the variable. Categorical variables are added to the argument `stratum`, and continuous variables are classed first. The regression coefficients are modeled across the strata. For example:

```{r eval=FALSE}
fitCPH2 <- cph(Surv(tenure, churn) ~ MonthlyCharges +
                       SeniorCitizen + Partner + Dependents +
                       StreamMov + Contract,
               stratum = "gender = Male",
               data = dataSurv, x = TRUE, y = TRUE, surv = TRUE)
```


- Another solution is to model **time-dependent coefficients** by dividing the time under consideration into different periods for which we assume the coefficients to be constant. 

To make sure that the model isn't **overfitted**, we validate the model using `rms::validate` to estimate the `R^2`. You can chose what fold you want to cross validate by setting the argument `method` to "crossvalidation" and the argument `B` to the number of folds you want (in the below example, it's 10):

```{r eval=FALSE}
validate(fitCHP1, 
         method = "crossvalidation",
         B = 10, pr = FALSE) #pr = F bc we don't want results printed after each cross val step
```

When analyzing the results of the above, the column `index.corrected` hold the `R^2` corrected for overfitting by cross validation. 

Predictions in survival analysis are not straightforward. Use `rms::survest` to estimate the probability that a certain customer (a separate, smaller dataframe called `oneNewData`) has not churned until a specific timepoint specified by the `times` argument:

```{r eval=FALSE}
str(survest(fitCHP1, newdata = oneNewData, times = 3)) # 3 months
```

When looking at the results of the above, the estimated survival probability `surv` tells us that the probability that the customer will not churn in 3 months. 

We can also estimate the survival curve for the new customer using `survfit`:

```{r eval=FALSE}
plot(survfit(fitCHP1,
             newdata = oneNewData))
```

To predict the expected time until churn, we print the `survfit` object and look at the median value:

```{r eval=FALSE}
print(survfit(fitCHP1, 
              newdata = oneNewData))
```


## Principal Component Analysis for CRM Data

PCA helps reduce dimensionality, or the number of correlated variables in your dataset. 

The first component is determined such that it covers as much of the observations' variance as possible. It is called **PC1** and is plotted on the x-axis.

The second component, **PC2** is determined such that it covers as much as possible of the remining variance, and it's plotted on the y-axis. This continues for the 3rd, 4th, etc. In the end there are as many components as variables, but you chose a subset of them. 

PCA helps to 

- handle multicolinearity

- create indices (by creating weighted averages) - just use PC1 as your index

- visualize and understand high-dimensional data 

PCA is an exploratory tool. It's not meant to test hypotheses about the structure of your data. 

### PCA Data

To get an overview of the correlation structure of the data, first compute and visualize the correlations of all variables with `cor` and `corrplot`:

```{r, eval=FALSE}
dataCustomers %>% cor() %>% corrplot()
```

Note that `cor` takes the whole dataset as input while `corrplot` takes the estimated correlations. Positive correlations are blue and negative are red/orange. 

## PCA Computation

Because PCA focuses on the variances of respective variables, variables with high variances are overrepresented in the resulting principal components. Differences in the measuring units also introduce a fake weight to the variables. This can be avoided by standardizing the variables using `scale` function which transforms them to having a mean of 0 and a variance of 1:

```{r eval=FALSE}
dataCustomers <- dataCustomer %>% scale() %>% as.data.frame()

#Check variances of all variables:
lapply(dataCustomers, var)
```

Once the variables have been standardized, compute the PCA with `stats::prcomp` which takes the entire dataset as input:

```{r eval=FALSE}
pcaCust <- prcomp(dataCustomers)

str(pcaCust, give.attr = FALSE)
```

The result is a list with 5 elements. The first element, `sdev` holds the standard deviations of the extracted components. There will be as many components are there are variables, and the first component has the highest standard deviation. The standard deviations of the remaining components become smaller each time as each component covers less of the original variance of the data. 

The variances of the components, the squared standard deviations, are called **eigenvalues**. They serve as a nice measure of the importance of the respective component (i.e. the higher the eigenvalue, the more important the component). 

If we divide the eigenvalues by the number of components, we get the proportion of variance that this component explains. 

The second element, `rotation`, of the `prcomp` object holds the **loadings** of the PCA. The loadings are the correlations between the original variables and the components and helps to interpret the components. 

The components of a PCA can be considered as something like a weighted average. Customer-specific characteristics are  weighted according to the loadings they have on the respective component and summed up. 

For an example, let's walk through calculating the value of the first component for the first customer in the dataset by hand. First we select the vector of the 1st customer's characteristics, then we multiply them with the vector containing the loadings on the first component from the `rotation` element. 

```{r eval=FALSE}
# Value on 1st component for 1st customer
sum(dataCustomers[1, ] * pcaCust$rotation[ ,1])
```

The values for each customer and component are stored in the element named `x`. 

### PCA Model Specification: Choosing the Right Number of Components

There are multiple ways to select the right number of components. In general, it's best practices to use a combination of all of them. 

#### 1. Set a minimum of the overall variance explained 

One way to decide is to **set a minimum of the overall variance explained**. Extract this by using the `summary` function on the `prcomp` object:

```{r eval=FALSE}
# Proportion of variance explained:
summary(pcaCust)
```

Look at the values for "Cumulative Proportion" and count the number of PCs (columns) where the value exceds your proportion threshold. In general, start with a proportion of about 70% (so count the number of columns with values > 0.70). 

#### 2. Use the Kaiser-Guttman criterion 

A second criterion is called the **Kaiser-Guttman criterion**. Here you only keep components with an eigenvalue larger than 1. 

```{r eval=FALSE}
pcaCust$sdev ^ 2
```

An eigenvalue smaller than 1 means that the component covers less variance than a single variable contributed, so as a result the component doesn't really help reduce dimensionality. 

#### 3. Visualize with `stats::screeplot`

The screeplot displays the variances (the eigenvalues) of all components in descending order. Look for the "elbow" and drop all the components to the right of it.

```{r eval=FALSE}
screeplot(pcaCust, type = "lines")
box()
abline(h = 1, lty = 2)
```


The `biplot` helps visualize how the variables and the components behave with respect to each other. The axes in the plot are made up from two principal components, in the `choices` argument. The arrows indicate the variables and the numbers indicate single observations. 

```{r eval=FALSE}
biplot(pcaCust, choices = 1:2, cex = 0.7)
```

When observing these plots, look where the observations are scattered - are they scattered mostly along x or y axis? 

If an arrow is nearly parallel to a component, it means that this variable loads high on the respective component. 

### Principal Components in a Regression Analysis

Remember, we use `stats::lm` to estimate a linear regression. The dot to the right of the ~ includes all variables contained in the dataset in the model:

```{r eval=FALSE}
mod1 <- lm(customerSatis ~ ., dataCustomer) #new variable, customer satisfaction
```

Then we compute the variance inflation factors using `car::vif`:

```{r eval=FALSE}
vif(mod1)
```

When analyzing these results, look for values above 5 or 10, which indicates strong multicollinearlity and renders the regression estimates unstable. To solve for this, we'll need to use the selected principal components as regressors instead. 

First construct a dataframe with customer satisfaction and the first 6 components:

```{r eval=FALSE}
# Create df with customer satisfaction and 1st 6 components (from PCA)
dataCustComponents <- cbind(dataCustomers[, "customerSatis"],
                            pcaCust$x[, 1:6]) %>%
        as.data.frame()
```

Then compute the linear model with the principal components as explanatory variables:

```{r eval=FALSE}
mod2 <- lm(customerSatis ~ ., dataCustComponents)
```

Because the componenta are by construction completely uncorrelated, all variance inflation factors should equal one now (i.e. `vif(mod2)` gives a 1 for each PC).

Extract the R^2 of the 1st and 2nd models:

```{r eval=FALSE}
summary(mod1)$adj.r.squared
summary(mod2)$adj.r.squared
```

The interpretation of the regression coefficients is now less straightforward bc they refer to the components and not the original variables. First display the results of the model:

```{r eval=FALSE}
summary(mod2)
```

One method that is often confused with PCA is **factor analysis**. While both are used as dimension reduction techniques, factor analysis identifies theoretical, latend contstructs like intelligence. These factors cannot be measured directly, but manifest themselves in measurable variables (i.e. the factors influence the observed values of the variables). Correlations between the variables are attributed to the common factors. Variance which cannot be explained by the factors are seen as error variance and is unrelated to the latent construct. 

Factor analysis is used for the development of questionnaires (EX. measuring certain personality traits).

In PCA, it is not the components that influence the variables, but the other way around: the components are composed of the variables. Hence you analyse how the items can be compressed. The remaining, uncovered variance is not seen as error variance, but as systematic variance you don't cover with the selected components. 

